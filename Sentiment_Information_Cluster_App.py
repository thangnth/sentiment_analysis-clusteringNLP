# rm -rf myenv
# python3 -m venv myenv
# source myenv/bin/activate
# pip3 install -r requirements.txt
# streamlit run Sentiment_Information_Cluster_App.py
import streamlit as st
import pickle
import re, regex
import numpy as np
from underthesea import word_tokenize, pos_tag, sent_tokenize
from scipy.sparse import csr_matrix, hstack
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import seaborn as sns
import io
import plotly.graph_objects as go  # <-- thÃªm Plotly cho radar chart tÆ°Æ¡ng tÃ¡c
# ========== Sidebar ==========
# ====== Sidebar MENU  ======
st.markdown("""
    <style>
        /* Thay Ä‘á»•i chiá»u rá»™ng sidebar */
        [data-testid="stSidebar"] {
            width: 350px;
        }

        /* Äiá»u chá»‰nh vÃ¹ng ná»™i dung chÃ­nh Ä‘á»ƒ trÃ¡nh Ä‘Ã¨ */
        [data-testid="stSidebar"] > div:first-child {
            width: 350px;
        }
    </style>
""", unsafe_allow_html=True)
with st.sidebar:
    with st.container():
        st.title("ğŸ“š Menu")
        menu_choice = st.radio("Chá»n chá»©c nÄƒng:", (
            "ğŸ“Œ Business Objective",
            "ğŸ—ï¸ Build Model",
            "ğŸ’¬ Sentiment Analysis",
            "ğŸ§© Information Clustering"
        ))
# ===== ThÃ´ng tin tÃ¡c giáº£ =====
st.sidebar.markdown("""
<div style='margin-top: auto; padding-bottom: 20px;'>
    <hr style="border: none; height: 1px; background-color: #ccc;">
    <h4>ğŸ“ TÃ¡c giáº£ Ä‘á»“ Ã¡n:</h4>
    ğŸ‘¨â€ğŸ“ <b>Nguyá»…n Ngá»c HuÃ¢n</b><br>
    ğŸ“§ nguyenngochuan992@gmail.com<br><br>
    ğŸ‘©â€ğŸ“ <b>Nguyá»…n T. Hoa Tháº¯ng</b><br>
    ğŸ“§ thangnth0511@gmail.com
</div>
""", unsafe_allow_html=True)

# ========== Load mÃ´ hÃ¬nh vÃ  vectorizer tá»« .pkl ==========
with open("vectorizer.pkl", "rb") as f:
    vectorizer = pickle.load(f)

with open("scaler.pkl", "rb") as f:
    scaler = pickle.load(f)

with open("model_lr.pkl", "rb") as f:
    model_lr = pickle.load(f)

with open("label_encoder.pkl", "rb") as f:
    le = pickle.load(f)

# ========== Load dictionary ==========
def load_dict_from_txt(path):
    with open(path, 'r', encoding='utf-8') as f:
        lines = f.read().splitlines()
    return dict(line.split('\t') for line in lines if '\t' in line)

def load_list_from_txt(path):
    with open(path, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f if line.strip()]

emoji_dict = load_dict_from_txt("files/emojicon.txt")
teen_dict = load_dict_from_txt("files/teencode.txt")
wrong_lst = load_list_from_txt("files/wrong-word.txt")
stopwords_lst = load_list_from_txt("files/vietnamese-stopwords.txt")
positive_words = load_list_from_txt("files/positive_VN.txt")
negative_words = load_list_from_txt("files/negative_VN.txt")
positive_emojis = load_list_from_txt("files/positive_emoji.txt")
negative_emojis = load_list_from_txt("files/negative_emoji.txt")
correct_dict = load_list_from_txt("files/phrase_corrections.txt")
english_dict = load_list_from_txt("files/english-vnmese.txt")
# ========== Tiá»n xá»­ lÃ½ ==========
def covert_unicode(txt):
    return txt.encode('utf-8').decode('utf-8')

def normalize_repeated_characters(text):
    return re.sub(r'(.)\1+', r'\1', text)

def process_text(text):
    document = text.lower().replace("â€™", '')
    document = regex.sub(r'\.+', ".", document)
    new_sentence = ''
    for sentence in sent_tokenize(document):
        sentence = ''.join(emoji_dict.get(c, c) for c in sentence)
        sentence = ' '.join(teen_dict.get(w, w) for w in sentence.split())
        pattern = r'(?i)\b[a-zÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘]+\b'
        sentence = ' '.join(regex.findall(pattern, sentence))
        sentence = ' '.join(w for w in sentence.split() if w not in wrong_lst)
        new_sentence += sentence + '. '
    return regex.sub(r'\s+', ' ', new_sentence).strip()

def process_postag_thesea(text):
    new_document = ''
    for sentence in sent_tokenize(text):
        sentence = sentence.replace('.', '')
        lst_word_type = ['N','Np','A','AB','V','VB','VY','R']
        sentence = ' '.join(
            word[0] if word[1] in lst_word_type else ''
            for word in pos_tag(word_tokenize(sentence, format="text"))
        )
        new_document += sentence + ' '
    return regex.sub(r'\s+', ' ', new_document).strip()

def remove_stopword(text):
    return regex.sub(r'\s+', ' ', ' '.join(w for w in text.split() if w not in stopwords_lst)).strip()

def count_sentiment_items(text):
    text = str(text).lower()
    pos_word = sum(1 for word in positive_words if word in text)
    pos_emoji = sum(text.count(emoji) for emoji in positive_emojis)
    neg_word = sum(1 for word in negative_words if word in text)
    neg_emoji = sum(text.count(emoji) for emoji in negative_emojis)
    return pos_word, neg_word, pos_emoji, neg_emoji

# ========== Dá»± Ä‘oÃ¡n ==========
def predict_sentiment(text_input, recommend_num):
    text = covert_unicode(text_input)
    text = normalize_repeated_characters(text)
    text = process_text(text)
    text = process_postag_thesea(text)
    text = remove_stopword(text)

    tfidf_vector = vectorizer.transform([text])
    pos_word, neg_word, pos_emoji, neg_emoji = count_sentiment_items(text_input)
    numeric_features = scaler.transform([[pos_word, neg_word, pos_emoji, neg_emoji]])
    recommend_feature = csr_matrix([[recommend_num]])

    final_features = hstack([tfidf_vector, csr_matrix(numeric_features), recommend_feature])
    y_pred = model_lr.predict(final_features)[0]
    label = le.inverse_transform([y_pred])[0]
    return label



# ========== CÃ¡c Trang á»¨ng Dá»¥ng ==========
# ğŸ”§ ChÃ¨n CSS toÃ n trang (ngay tá»« Ä‘áº§u file)
st.set_page_config(page_title="PhÃ¢n tÃ­ch cáº£m xÃºc vÃ  phÃ¢n cá»¥m", layout="wide")
st.image("images/NaturalLanguageProcessing.png", width=1000)
st.markdown("""
    <style>
        .stMain {
            padding-left: 5% !important ;
            padding-right: 5% !important;
           
        }
        p, li {
            font-size:18px !important;
            text-align: justify;
            }
    </style>
""", unsafe_allow_html=True)


# Header áº£nh dÃ¹ng chung
if menu_choice == "ğŸ“Œ Business Objective":
    # st.title("ğŸ“Œ Giá»›i thiá»‡u Ä‘á»“ Ã¡n: Sentiment Analysis & Information Clustering")
    col_sentiment_intro, col_sentiment_img = st.columns([2, 1])  # tá»· lá»‡ chiá»u rá»™ng 2:1
    with col_sentiment_intro:
        st.markdown("""
            <p> 
                <strong> Sentiment Analysis </strong> vÃ  <strong> Clustering </strong> 
                lÃ  cÃ¡c ká»¹ thuáº­t trong lÄ©nh vá»±c <strong> xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (Natural Language Processing - NLP) </strong>. </p>
            <p>
                <strong> Sentiment Analysis </strong>, hay cÃ²n gá»i lÃ  <em> phÃ¢n tÃ­ch cáº£m xÃºc </em>, <em> phÃ¢n tÃ­ch quan Ä‘iá»ƒm </em> , <em> Opinion Mining </em>) lÃ  ká»¹ thuáº­t trong há»c mÃ¡y cÃ³ giÃ¡m sÃ¡t , nháº±m xÃ¡c Ä‘á»‹nh, trÃ­ch xuáº¥t vÃ  Ä‘á»‹nh lÆ°á»£ng cÃ¡c thÃ´ng tin chá»§ quan tá»« vÄƒn báº£n. NÃ³i má»™t cÃ¡ch Ä‘Æ¡n giáº£n, Ä‘Ã¢y lÃ  quÃ¡ trÃ¬nh mÃ¡y tÃ­nh hiá»ƒu vÃ  phÃ¢n loáº¡i cáº£m xÃºc, thÃ¡i Ä‘á»™, Ã½ kiáº¿n hay quan Ä‘iá»ƒm Ä‘Æ°á»£c thá»ƒ hiá»‡n trong ná»™i dung vÄƒn báº£n.</p>
            </p>
            <p>
                <strong> Clustering </strong>, hay cÃ²n gá»i lÃ  <em> phÃ¢n cá»¥m </em>, lÃ  má»™t ká»¹ thuáº­t trong há»c mÃ¡y khÃ´ng giÃ¡m sÃ¡t, dÃ¹ng Ä‘á»ƒ nhÃ³m cÃ¡c Ä‘á»‘i tÆ°á»£ng dá»¯ liá»‡u tÆ°Æ¡ng tá»± nhau vÃ o cÃ¹ng má»™t cá»¥m (cluster). Má»¥c tiÃªu lÃ  Ä‘á»ƒ cÃ¡c Ä‘á»‘i tÆ°á»£ng trong cÃ¹ng má»™t cá»¥m cÃ³ tÃ­nh tÆ°Æ¡ng Ä‘á»“ng cao, trong khi cÃ¡c cá»¥m khÃ¡c nhau thÃ¬ cÃ³ tÃ­nh khÃ¡c biá»‡t. </p>
                    
        """, unsafe_allow_html=True)
    with col_sentiment_img:
        st.image("images/img1_sentimen_intro.png", use_container_width=True) 
    st.markdown("""
           <div class="info-box">
           </div>
        """, unsafe_allow_html=True)
    st.markdown("""
            <h4> ğŸ¯ Má»¥c tiÃªu Ä‘á»“ Ã¡n </h4>
                <p> ğŸ“ Sá»­ dá»¥ng ká»¹ thuáº­t Sentiment Analysis xÃ¢y dá»±ng á»©ng dá»¥ng phÃ¢n tÃ­ch cáº£m xÃºc tá»« cÃ¡c Ä‘Ã¡nh giÃ¡ cÃ´ng ty cá»§a nhá»¯ng ngÆ°á»i tham gia kháº£o sÃ¡t á»Ÿ trang ITViec. Tá»« Ä‘Ã³: 
                    <ul style= "list-style-type: none" >
                        <li> ğŸ“ Doanh nghiá»‡p xÃ¢y dá»±ng cÃ¡c bÃ¡o cÃ¡o ná»™i bá»™ Ä‘á»ƒ cáº£i tiáº¿n mÃ´i trÆ°á»ng lÃ m viá»‡c. </li>
                        <li> ğŸ“ Tá»± Ä‘á»™ng phÃ¡t hiá»‡n lÃ n sÃ³ng Ä‘Ã¡nh giÃ¡ tiÃªu cá»±c, giÃºp cáº£nh bÃ¡o sá»›m khá»§ng hoáº£ng truyá»n thÃ´ng cho doanh nghiá»‡p. </li>
                        <li> ğŸ“ Gá»£i Ã½ cÃ´ng ty phÃ¹ há»£p cho á»©ng viÃªn. </li>
                    </ul>
                </p>
                <p> ğŸ“ Sá»­ dá»¥ng ká»¹ thuáº­t Clustering xÃ¢y dá»±ng á»©ng dá»¥ng phÃ¢n cá»¥m cÃ¡c Ä‘Ã¡nh giÃ¡ cÃ´ng ty cá»§a nhá»¯ng ngÆ°á»i tham gia kháº£o sÃ¡t á»Ÿ trang ITViec. Tá»« Ä‘Ã³: 
                    <ul style= "list-style-type: none" >
                        <li> ğŸ“ GiÃºp doanh nghiá»‡p tÃ¬m ra cÃ¡c váº¥n Ä‘á» ná»•i báº­c cá»§a cÃ´ng ty : lÆ°Æ¡ng tháº¥p, cÃ´ng nghá»‡ cÅ©, quáº£n lÃ½ tá»‡... </li>
                        <li> ğŸ“ GiÃºp á»©ng viÃªn so sÃ¡nh cÃ¡c cÃ´ng ty cÃ³ nhÃ³m vÄƒn hoÃ¡ khÃ¡c nhau nháº±m tÃ¬m kiáº¿m mÃ´i trÆ°á»ng lÃ m viá»‡c phÃ¹ há»£p. </li>
                        <li> ğŸ“ KhÃ¡m phÃ¡ cÃ¡c insight áº©n tá»« cÃ¡c Ä‘Ã¡nh giÃ¡ khÃ´ng dÃ¡n nhÃ£n.
                    </ul>
                </p>
    """, unsafe_allow_html=True)
    st.markdown("""
        <h4> ğŸ”‘ CÃ¡c phÆ°Æ¡ng phÃ¡p xÃ¢y dá»±ng á»©ng dá»¥ng: </h4>
        <p> ğŸ“ Sentiment Analysis:
            <ul style= "list-style-type: none" >
                <li> ğŸ“ Sá»­ dá»¥ng <strong> Tfidf </strong> vectorize dá»¯ liá»‡u vÄƒn báº£n. </li>
                <li> ğŸ“ Sá»­ dá»¥ng <strong> MinMaxScaler </strong> Ä‘á»ƒ chuáº©n hoÃ¡ dá»¯ liá»‡u kiá»ƒu sá»‘. </li>
                <li> ğŸ“ Sá»­ dá»¥ng <strong> hstack (Horizontal Stack) </strong> Ä‘á»ƒ ná»‘i cÃ¡c ma tráº­n dense vÃ  spare thÃ nh ma tráº­n Ä‘áº·c trÆ°ng X. </li>
                <li> ğŸ“ Sá»­ dá»¥ng <strong> Label Encoder </strong> Ä‘á»ƒ mÃ£ hoÃ¡ nhÃ£n Sentiment (positive, neutral, negative). </li>
                <li> ğŸ“ Sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh phÃ¢n loáº¡i cá»§a thÆ° viá»‡n sklearn : <strong> Logistic Regression, Random Forest, Decision Tree </strong> Ä‘á»ƒ huáº¥n luyá»‡n train data.</li>
            </ul>
        </p>
        <p> ğŸ“ Clustering Analysis:
            <ul style= "list-style-type: none" >
                <li> ğŸ“ Sá»­ dá»¥ng <strong> CountVectorizer </strong> vectorize dá»¯ liá»‡u vÄƒn báº£n.</li>
                <li> ğŸ“ Sá»­ dá»¥ng biá»ƒu Ä‘á»“ <strong> wordcould </strong> biá»ƒu diá»…n cÃ¡c cá»¥m tá»« phá»• biáº¿n. </li>
                <li> ğŸ“ Sá»­ dá»¥ng mÃ´ hÃ¬nh phÃ¢n chá»§ Ä‘á» (topic modeling)  <strong> LDA </strong> Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c chá»§ Ä‘á» tÃ¬m áº©n tá»« táº­p vÄƒn báº£n.  </li>
                <li> ğŸ“ Sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh <strong> Kmean, Birch, SpectralClustering,  AgglomerativeClustering  </strong> Ä‘á»ƒ phÃ¢n cá»¥m. </li>
                <li> ğŸ“ Trá»±c quan hoÃ¡ phÃ¢n cá»¥m báº±ng biá»ƒu Ä‘á»“ <strong> Scatter </strong> káº¿t há»£p giáº£m chiá»u dá»¯ liá»‡u báº±ng <strong> phÆ°Æ¡ng phÃ¡p PCA </strong>. </li>
            </ul>
        </p>
    """, unsafe_allow_html=True)

elif menu_choice == "ğŸ—ï¸ Build Model":
    st.title("XÃ¢y dá»±ng mÃ´ hÃ¬nh")
    st.write("### I. Sentiment Analysis")
    st.write("##### 1. Data EDA")
    st.image("images/Sentiment_EDA.JPG")
    st.write("##### 2. Visualization")
    st.image("images/sentiment_distributed_data.JPG")
    st.write("##### 3. Build model and Evaluation")
    st.write("###### - Huáº¥n luyá»‡n mÃ´ hÃ¬nh phÃ¢n loáº¡i Logistic Regression , Random Forest , Decision Tree")
    st.write("###### - ÄÃ¡nh giÃ¡ káº¿t quáº£ dá»±a trÃªn Presicion , ReCall , F1-Score , Accuracy")
    st.image("images/sentiment_evaluation.JPG")
    st.write("###### Confusion Matrix")
    st.image("images/Confusion Matrix.JPG")
    st.markdown("Chá»n mÃ´ hÃ¬nh <span style='color: red; font-weight: bold; text-decoration: underline'>Logistic Regression</span> lÃ  tá»‘i Æ°u nháº¥t.",
    unsafe_allow_html=True)
    st.write("### II. Information Clustering")
    st.write("##### 1. Data EDA")
    st.image("images/Clustering_EDA.JPG")
    st.write("##### 2. Visualization")
    st.image("images/Cluster_wordcloud.JPG")
    st.write("##### 3. Build model and Evaluation")
    st.write("###### - Huáº¥n luyá»‡n mÃ´ hÃ¬nh phÃ¢n cá»¥m vá»›i cÃ¡c thuáº­t toÃ¡n KMeans, AgglomerativeClustering, SpectralClustering, Birch")
    st.write("###### - ÄÃ¡nh giÃ¡ káº¿t quáº£ dá»±a trÃªn Sihouette score")
    st.image("images/k_evaluation.JPG", width=600)
    st.write("###### Trá»±c quan hoÃ¡ Elbow theo Sihouette score")
    st.image("images/ellbow.JPG")
    st.write("###### Trá»±c quan hoÃ¡ phÃ¢n cá»¥m")
    st.image("images/Cluster_distributed.JPG")
    st.markdown(" Káº¿t luáº­n : Chá»n mÃ´ hÃ¬nh <span style='color: red; font-weight: bold; text-decoration: underline'>KMeans</span> vá»›i k=4 lÃ  mÃ´ hÃ¬nh tá»‘i Æ°u nháº¥t vÃ¬:",unsafe_allow_html=True)
    st.markdown(""" 
    - Silhouette Score â‰ˆ  0.775 cao nháº¥t vá»›i k=4, ráº¥t á»•n Ä‘á»‹nh.
    - CÃ¡c Ä‘iá»ƒm cÃ²n láº¡i giáº£m nháº¹ nhÆ°ng váº«n khÃ¡ cao â†’ á»•n Ä‘á»‹nh tá»‘t.
    - Biá»ƒu Ä‘á»“ phÃ¢n cá»¥m (LDA + KMeans): NhÃ³m dá»¯ liá»‡u Ä‘Æ°á»£c chia rÃµ rÃ ng, trá»±c quan.
    - Ranh giá»›i giá»¯a cÃ¡c cá»¥m rÃµ rÃ ng, gáº§n nhÆ° khÃ´ng cÃ³ Ä‘iá»ƒm chá»“ng láº¥n.
    """)
                
    st.write("##### 4. Interpreting and Visualizing Cluster Analysis Results")
    st.write("###### âœ… Chá»§ Ä‘á» #1: LÆ°Æ¡ng vÃ  cháº¿ Ä‘á»™ Ä‘Ã£i ngá»™ : Cá»¥m nÃ y nháº¥n máº¡nh vá» cÃ¡c yáº¿u tá»‘ vá» lÆ°Æ¡ng vÃ  phÃºc lá»£i , sá»± cÃ¢n báº±ng giá»¯a cÃ´ng viá»‡c vÃ  cuá»™c sá»‘ng cÃ¡ nhÃ¢n, Ä‘áº·c  biá»‡t cÃ³ Ä‘á» cáº­p Ä‘áº¿n váº¥n Ä‘á» báº¥t cáº­p lÃ  lÆ°Æ¡ng_cháº­m")
    st.write("###### ğŸ”‘ Key words: lÆ°Æ¡ng_thÆ°á»Ÿng, lÆ°Æ¡ng_tá»‘t, lÆ°Æ¡ng_cháº­m, cÃ¢n_báº±ng_cuá»™c_sá»‘ng, cuá»™c_sá»‘ng_cÃ´ng_viá»‡c, lÃ m_viá»‡c_linh_hoáº¡t, sá»©c_khoáº». sá»©c_khoáº»")
    st.image("images/wordcloud_0.JPG")
    st.write("######  âœ… Chá»§ Ä‘á» #2:  MÃ´i trÆ°á»ng lÃ m viá»‡c chuyÃªn nghiá»‡p: á»©ng viÃªn quan tÃ¢m Ä‘áº¿n mÃ´i trÆ°á»ng lÃ m viá»‡c chuyÃªn nghiá»‡p vá»›i cÃ¡c dá»± Ã¡n lá»›n , cÃ´ng ty lá»›n. Cá»¥m nÃ y cÃ³ Ä‘á» cáº­p váº¥n Ä‘á» báº¥t cáº­p lÃ  â€˜cÃ´ng nghá»‡ cÅ©â€™")
    st.write("###### ğŸ”‘ Key words: Ä‘á»“ng_nghiá»‡p_vui_váº», Ä‘á»“ng_nghiá»‡p_thÃ¢n_thiá»‡n, nhÃ _quáº£n_lÃ½_thÃ¢n_thiá»‡n, mÃ´i_trÆ°á»ng_lÃ m_viá»‡c_vui_váº», quy_trÃ¬nh_lÃ m_viá»‡c, lÆ°Æ¡ng_Ä‘áº§y_Ä‘á»§, khá»‘i_lÆ°á»£ng_cÃ´ng_viá»‡c, cháº¿_Ä‘á»™_phÃºc_lá»£i, cháº¿_Ä‘á»™_Ä‘Ã£i_ngá»™")
    st.image("images/wordcloud_1.JPG")
    st.write("######  âœ… Chá»§ Ä‘á» #3: TrÃ£i nghiá»‡m khÃ´ng gian lÃ m viá»‡c hiá»‡n Ä‘áº¡i vÃ  mÃ´i trÆ°á»ng lÃ m viá»‡c tráº» trung")
    st.write("###### ğŸ”‘ Key words: dcÃ´ng_ty_lá»›n, mÃ´i_trÆ°á»ng_lÃ m_viá»‡c_chuyÃªn_nghiá»‡p, cÃ´ng_ty_tá»‘t, dá»±_Ã¡n_lá»›n, mÃ´i_trÆ°á»ng_lÃ m_viá»‡c_nÄƒng_Ä‘á»™ng, cÃ´ng_nghá»‡_cÅ©.")
    st.image("images/wordcloud_2.JPG")
    st.write("######  âœ… Chá»§ Ä‘á» #4: Má»‘i quan há»‡ trong cÃ´ng viá»‡c , cÆ¡ sá»Ÿ váº­t cháº¥t , quy trÃ¬nh lÃ m viá»‡c, cháº¿ Ä‘á»™ lÃ m viá»‡c.")
    st.write("###### ğŸ”‘ Key words: Ä‘á»“ng_nghiá»‡p_vui_váº», Ä‘á»“ng_nghiá»‡p_thÃ¢n_thiá»‡n, nhÃ _quáº£n_lÃ½_thÃ¢n_thiá»‡n, mÃ´i_trÆ°á»ng_lÃ m_viá»‡c_vui_váº», quy_trÃ¬nh_lÃ m_viá»‡c, lÆ°Æ¡ng_Ä‘áº§y_Ä‘á»§, khá»‘i_lÆ°á»£ng_cÃ´ng_viá»‡c, cháº¿_Ä‘á»™_phÃºc_lá»£i, cháº¿_Ä‘á»™_Ä‘Ã£i_ngá»™")
    st.image("images/wordcloud_3.JPG")
elif menu_choice == "ğŸ’¬ Sentiment Analysis":
    st.title("ğŸ’¬ á»¨ng dá»¥ng phÃ¢n tÃ­ch cáº£m xÃºc review cÃ´ng ty")

    input_text = st.text_area("âœï¸ Nháº­p cÃ¢u Ä‘Ã¡nh giÃ¡ cá»§a báº¡n:", height=150)
    recommend_input = st.checkbox("âœ… Báº¡n cÃ³ recommend cÃ´ng ty nÃ y khÃ´ng?", value=False)
    recommend_num = 1 if recommend_input else 0

    if st.button("ğŸš€ Dá»± Ä‘oÃ¡n cáº£m xÃºc"):
        if not input_text.strip():
            st.warning("â›” Vui lÃ²ng nháº­p ná»™i dung review!")
        else:
            with st.spinner("ğŸ” Äang xá»­ lÃ½..."):
                result = predict_sentiment(input_text, recommend_num)
            st.success(f"âœ… Káº¿t quáº£ dá»± Ä‘oÃ¡n: **{result.upper()}**")
    st.markdown("---")
    st.subheader("ğŸ“¥ PhÃ¢n tÃ­ch file Ä‘Ã¡nh giÃ¡ hÃ ng loáº¡t")
    uploaded_file = st.file_uploader("Táº£i lÃªn file Excel (.xlsx) cÃ³ cá»™t 'Name', 'review' vÃ  'recommend'", type=["xlsx"])

    if uploaded_file:
        try:
            df_file = pd.read_excel(uploaded_file, engine="openpyxl")
            if 'review' not in df_file.columns or 'recommend' not in df_file.columns or 'Name' not in df_file.columns:
                st.error("âŒ File cáº§n cÃ³ Ä‘á»§ 3 cá»™t 'Name', 'review' vÃ  'recommend'. Vui lÃ²ng kiá»ƒm tra láº¡i.")
            else:
                with st.spinner("ğŸ” Äang xá»­ lÃ½ dá»± Ä‘oÃ¡n hÃ ng loáº¡t..."):
                    df_file['sentiment'] = df_file.apply(lambda row: predict_sentiment(row['review'], row['recommend']), axis=1)

                st.success("âœ… PhÃ¢n tÃ­ch hoÃ n táº¥t! Báº¡n cÃ³ thá»ƒ táº£i file káº¿t quáº£ vÃ  xem thá»‘ng kÃª theo tá»«ng ngÆ°á»i Ä‘Ã¡nh giÃ¡.")
                output = io.BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    df_file.to_excel(writer, index=False)
                st.download_button(
                    label="ğŸ“¥ Táº£i káº¿t quáº£ (.xlsx)",
                    data=output.getvalue(),
                    file_name="sentiment_result.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

                st.markdown("---")
                st.subheader("ğŸ“Š Thá»‘ng kÃª cáº£m xÃºc theo ngÆ°á»i Ä‘Ã¡nh giÃ¡ (Name)")
                df_counts = df_file.groupby(['Name', 'sentiment']).size().unstack(fill_value=0)
                df_percent = df_counts.div(df_counts.sum(axis=1), axis=0) * 100
                st.dataframe(df_percent.style.format("{:.1f}%"))

                fig, ax = plt.subplots(figsize=(10, 5))
                df_percent[['positive', 'neutral', 'negative']].plot(
                    kind='bar', stacked=True, ax=ax,
                    color=['red', 'skyblue', 'blue']
                )
                ax.set_ylabel("Tá»· lá»‡ (%)")
                ax.set_title("Tá»· lá»‡ cáº£m xÃºc theo tá»«ng ngÆ°á»i Ä‘Ã¡nh giÃ¡")
                ax.set_xticklabels(ax.get_xticklabels(),rotation=360)
                st.pyplot(fig)

        except Exception as e:
            st.error(f"Lá»—i xá»­ lÃ½ file: {e}")
elif menu_choice == "ğŸ§© Information Clustering":
    st.title("ğŸ§© Information Clustering")
    
    try:
        df = pd.read_csv("clustered_reviews.csv", encoding='utf-8')

        company_list = sorted(df["Company Name"].dropna().unique())
        selected_company = st.selectbox("ğŸ” Chá»n cÃ´ng ty Ä‘á»ƒ phÃ¢n tÃ­ch:", company_list)
        df = df[df["Company Name"] == selected_company]
        # Radar chart cho cÃ¡c thuá»™c tÃ­nh Ä‘Ã¡nh giÃ¡
        st.markdown("---")
        st.subheader("ğŸ“ˆ ÄÃ¡nh giÃ¡ tá»•ng quan theo cÃ¡c khÃ­a cáº¡nh")
        radar_cols = [
            "Salary & benefits",
            "Training & learning",
            "Management cares about me",
            "Culture & fun",
            "Office & workspace"
        ]
        if all(col in df.columns for col in radar_cols):
            avg_scores = df[radar_cols].mean().values

            fig = go.Figure()
            fig.add_trace(go.Scatterpolar(
                r=avg_scores,
                theta=radar_cols,
                fill='toself',
                name=selected_company,
                text=[f"{col}: {score:.2f}" for col, score in zip(radar_cols, avg_scores)],
                hoverinfo="text",
                marker=dict(color='royalblue')
            ))

            fig.update_layout(
                polar=dict(radialaxis=dict(visible=True, range=[0, 5])),
                showlegend=False,
                title=f"Biá»ƒu Ä‘á»“ Radar Ä‘Ã¡nh giÃ¡ - {selected_company}"
            )

            st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("âš ï¸ Dá»¯ liá»‡u khÃ´ng Ä‘áº§y Ä‘á»§ Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“ radar.")

        # Bar chart & pie chart cho cá»™t Sentiment
        if "Sentiment" in df.columns:
            st.markdown("---")
            st.subheader("ğŸ“Š PhÃ¢n phá»‘i cáº£m xÃºc tá»« Ä‘Ã¡nh giÃ¡")

            sentiment_counts = df['Sentiment'].value_counts()

            color_map = {
                'positive': '#90ee90',
                'neutral': '#87cefa',
                'negative': '#ffb6c1'
            }
            colors = [color_map.get(sent, 'gray') for sent in sentiment_counts.index]

            col1, col2 = st.columns(2)
            with col1:
                fig_bar, ax = plt.subplots()
                sentiment_counts.plot(kind='bar', color=colors, ax=ax)
                ax.set_title("Sá»‘ lÆ°á»£ng bÃ¬nh chá»n theo cáº£m xÃºc")
                ax.set_xlabel("Sentiment")
                ax.set_ylabel("Sá»‘ lÆ°á»£ng")
                ax.set_xticklabels(sentiment_counts.index, rotation=0)
                st.pyplot(fig_bar)

            with col2:
                fig_pie, ax = plt.subplots()
                sentiment_counts.plot(kind='pie', autopct='%1.1f%%', colors=colors, ax=ax)
                ax.set_title("Tá»· lá»‡ cáº£m xÃºc theo pháº§n trÄƒm")
                ax.set_ylabel("")
                st.pyplot(fig_pie)        
    
        # Vector hÃ³a vÄƒn báº£n
        vectorizer_cluster = CountVectorizer(max_features=1000)
        X_vec = vectorizer_cluster.fit_transform(df["binh_luan"])

        # PhÃ¢n cá»¥m vá»›i KMeans
        kmeans = KMeans(n_clusters=4, random_state=42)
        df["Cluster"] = kmeans.fit_predict(X_vec)

        def get_top_words_in_cluster(dataframe, cluster_id, n_words=10):
            cluster_text = " ".join(dataframe[dataframe['cluster'] == cluster_id]['clean_text'].dropna().astype(str).tolist())
            if not cluster_text:
                return []
            vectorizer = CountVectorizer(max_features=n_words)
            X = vectorizer.fit_transform([cluster_text])
            word_counts = X.sum(axis=0).A1
            words = vectorizer.get_feature_names_out()
            word_freq = pd.Series(word_counts, index=words).sort_values(ascending=False)
            return word_freq.index.tolist(), cluster_text

        cluster_stats = df['cluster'].value_counts().sort_index()
        st.markdown(f"### ğŸ“Š CÃ´ng ty `{selected_company}` cÃ³ cÃ¡c cá»¥m nhÆ° sau:")
                # HÃ m láº¥y tá»« khÃ³a toÃ n cÃ´ng ty
        def get_top_keywords_company(df, n_keywords=20):
            all_text = " ".join(df['clean_text'].dropna().astype(str).tolist())
            if not all_text:
                return []
            vectorizer = CountVectorizer()
            X = vectorizer.fit_transform([all_text])
            words = vectorizer.get_feature_names_out()
            counts = X.toarray().flatten()
            word_freq = pd.Series(counts, index=words).sort_values(ascending=False)
            return word_freq.head(n_keywords)
            
        for cluster_id in cluster_stats.index:
            top_words, cluster_text = get_top_words_in_cluster(df, cluster_id)
            st.markdown(f"- Cá»¥m **#{cluster_id}**: ğŸ”‘ Tá»« khÃ³a: _{', '.join(top_words)}_")
            if cluster_text:
                wordcloud = WordCloud(width=1000, height=500, background_color='white',max_words=10).generate(cluster_text)
                st.image(wordcloud.to_array(), caption=f"WordCloud cho cá»¥m #{cluster_id}", use_container_width=True)
                            # === Thá»‘ng kÃª tá»•ng há»£p cÃ¡c tá»« khÃ³a tá»« táº¥t cáº£ cá»¥m ===
        # all_keywords = []
        # for cluster_id in cluster_stats.index:
        #     top_words, _ = get_top_words_in_cluster(df, cluster_id)
        #     all_keywords.extend(top_words)

        # if all_keywords:
        #     st.markdown("---")
        #     st.markdown("### ğŸ§  Tá»•ng há»£p váº¥n Ä‘á» ná»•i báº­t tá»« cÃ¡c cá»¥m Ä‘Ã¡nh giÃ¡")

        #     keyword_counts = pd.Series(all_keywords).value_counts()
        #     top_keywords = keyword_counts.head(10)

        #     for idx, (kw, count) in enumerate(top_keywords.items(), 1):
        #         st.markdown(f"{idx}. **{kw}** â€” xuáº¥t hiá»‡n trong **{count} cá»¥m**")

        #     # Optional: váº½ biá»ƒu Ä‘á»“ tá»« khÃ³a ná»•i báº­t
        #     fig, ax = plt.subplots()
        #     sns.barplot(x=top_keywords.values, y=top_keywords.index, palette="viridis", ax=ax)
        #     ax.set_title("ğŸ“ˆ Tá»« khÃ³a ná»•i báº­t nháº¥t trong cÃ¡c cá»¥m")
        #     ax.set_xlabel("Sá»‘ cá»¥m xuáº¥t hiá»‡n")
        #     ax.set_ylabel("Tá»« khÃ³a")
        #     st.pyplot(fig) 
                # Tá»« khÃ³a ná»•i báº­t toÃ n cÃ´ng ty
        st.markdown("---")
        st.subheader("ğŸ“Œ Tá»« khÃ³a ná»•i báº­t toÃ n cÃ´ng ty")
        top_keywords = get_top_keywords_company(df, n_keywords=10)
        st.write("Top 10 tá»« khÃ³a phá»• biáº¿n:")
        st.markdown(", ".join(top_keywords.index))

        wordcloud_all = WordCloud(width=1000, height=500, background_color='white',max_words=10).generate(" ".join(df['clean_text']))
        st.image(wordcloud_all.to_array(), caption=f"WordCloud toÃ n bá»™ review cÃ´ng ty {selected_company}", use_container_width=True)            
    except Exception as e:
        st.error(f"Lá»—i Ä‘á»c hoáº·c xá»­ lÃ½ dá»¯ liá»‡u: {e}")


